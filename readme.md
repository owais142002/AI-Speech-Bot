Overview
This project consists of a React-based frontend that listens to user queries, sends them to a Flask backend, and receives responses generated by a locally set up Llama3 language model. The backend handles the processing and communication with the LLM to provide intelligent answers to the queries.

Project Structure
frontend/: Contains the React application code.
backend/: Contains the Flask application code and configuration for the Llama3 language model.
Getting Started
Prerequisites
Node.js and npm installed on your machine.
Python and pip installed on your machine.
Llama3 language model set up locally.
Running the Frontend
Navigate to the frontend directory:

bash
Copy code
cd frontend
Install the required npm packages:

bash
Copy code
npm install
Start the React development server:

bash
Copy code
npm start
The frontend should now be running on http://localhost:3000.

Running the Backend
Navigate to the backend directory:

bash
Copy code
cd backend
Install the required Python packages:

bash
Copy code
pip install -r requirements.txt
Start the Flask server:

bash
Copy code
flask run
The backend should now be running on http://localhost:5000.

Using the Application
Open the frontend application in your web browser (http://localhost:3000).
Interact with the application by pressing the voice button to start listening to your query.
The query will be sent to the Flask backend, processed by the Llama3 language model, and the response will be displayed on the frontend.
Contributing
Feel free to contribute to this project by submitting issues or pull requests. Contributions are welcome!

License
This project is licensed under the MIT License. See the LICENSE file for details.

Acknowledgments
React
Flask
Llama3
